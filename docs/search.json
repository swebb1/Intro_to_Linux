[
  {
    "objectID": "01-start.html",
    "href": "01-start.html",
    "title": "Getting Started",
    "section": "",
    "text": "In this section you will learn how to log in to a remote server and run some simple commands. You will also learn how to find out about commands, and how to use bash efficiently.\n\nStarting a Terminal and logging in to the remote server\n\nThe process of starting a terminal depends on the operating system that you are using:\n\nOn Mac OS X or Ubuntu Linux, simply open the ‘Terminal’ application\nOn Windows, you can install MobaXTerm\n\nYou can log in to the server by typing the following command in the Terminal, substituting [USERNAME] for your account username:\n\nssh [USERNAME]@bifx-core1.bio.ed.ac.uk\n\nIf you are using MobaXTerm, an alternative way of logging into the server is shown in the MobaXTerm demo.\nOnce you have typed in your password, you should see some welcome text and a prompt that looks something like this:\n\n[USERNAME]@bifx-core1:~$\n\nThis prompt shows that you are logged in to the bifx-core1 bioinformatics server, and that you are in your home directory (which is referred to using the ~ character on Linux systems).\nPlease see our website for more specific instructions on using the DRP-HCB bioinformatics servers.\n\n\nRunning basic commands in bash\n\n\n\n Key Points\n\n\nCommands in bash can be categorised as either:\n\nbuilt in commands (or builtins), which are part of the bash shell, or\ninstalled programs, which could be either the basic GNU core utilities (or coreutils), included in most Linux distributions, or other programs, such as specialised bioinformatics tools\n\n\n\n\nIn order to run a command in bash you type the command and then press the return key to execute it. For example, try typing date at the prompt and pressing return. You should get something like this:\n\n[USERNAME]@bifx-core1:~$ date\nWed 31 Jan 09:21:43 GMT 2024\n[USERNAME]@bifx-core1:~$ \n\nAs you can see, the output of the date command is printed on the command line, and then another prompt is shown.\nAs well as the command name, a command can include ‘options’ and ‘arguments’. For example, try typing date -d '25 Dec' +%j at the command line:\n\n[USERNAME]@bifx-core1:~$ date -d '25 Dec' +%j\n360\n[USERNAME]@bifx-core1:~$ \n\nIn this example, the command includes the following elements:\n\nthe command name date, which always comes first\none option -d '25 Dec', which has a name -d (option names always start with at least one ‘-’), and a value '25 Dec'\none argument +%j\n\n\nOptions and arguments\nOptions always have a name, but do not always need to have an associated value. Options that don’t have a value are called flags, and are often used as switches for different kinds of functionality. Unlike options, arguments don’t have names. Instead they are identified by their position.\nIn general, the values of options and arguments are text strings, which are passed directly to the command. If a value contains spaces, you can surround it with quote marks, as in the example above, so that it is recognised as a single value. You can also use the output of one command as an argument to another by enclosing it in backticks ``, as in the following example:\n\n[USERNAME]@bifx-core1:~$ echo date\ndate\n[USERNAME]@bifx-core1:~$ echo `date`\nSat 14 Nov 11:25:11 GMT 2020\n[USERNAME]@bifx-core1:~$ \n\nThe above example uses the echo command, which outputs the text value of its arguments. In the first instance it simply prints the word ‘date’. In the second instance, it runs the command within the backticks first, then prints the output of this command.\n\n\n\nFinding out about bash commands\n\n\n\n Key Points\n\n\nThe bash shell provides a number of different ways to access information about commands, in particular:\n\nThe man command\nThe info command\n--help and -h flags\n\n\n\n\nWhile the use of options and arguments makes commands more powerful and flexible, they can be difficult to remember, and can vary between different versions of the command. For this reason, it is useful to be able to find information about commands easily.\n\nThe man command\nThis displays a short manual page for a single command. To see the manual page for the date command simply type man date.\n\nYou can scroll through the man page using space, and type q to quit\n\nYou can learn more about different commands by typing h\n\nTo learn about bash builtins, type man builtins\n\n\n\nThe info command\nThis displays a more comprehensive hyperlinked manual, which provides detailed information about the GNU coreutils. Simply type info to see the info documentation, and type H in info to find out how to navigate the documentation. Press q to quit.\n\n\n--help and -h flags\nSome programs may not have any documentation that is accessible using the man or info commands. In this case, they may follow the convention of including --help or -h flags, which will show some usage information. As an example, we can try to find help information about bedtools, which is a well known bioinformatics tool that we will look at later in the course:\n\n[USERNAME]@bifx-core1:~$ man bedtools\nNo manual entry for bedtools\n[USERNAME]@bifx-core1:~$ info bedtools\ninfo: No menu item 'bedtools' in node '(dir)Top'\n[USERNAME]@bifx-core1:~$ bedtools --help\nbedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.30.0\n...\n\n\n\n Challenge:\n\nWe saw the command “date -d ‘25 Dec’ +%j” in the previous section. Can you use the man command to explain what it does?\nCan you easily check how many days we have had this year?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nRun man date, and look for the -d flag. The documentation explains that -d tells date to display the time described by the value of the option, rather than the current time. Now look up %j in the documentation. The relevant line says ‘%j day of year (001..366)’. So the command tells you which day of the year it will be on the 25th of December.\n\nTo check the day of the year for today just run:\ndate +%j\n\n\n\n\n\n\nUsing bash efficiently\n\n\n\n Key Points\n\n\nWorking with long commands in bash can be laborious, but luckily there are a few tricks that you can use to make things easier. These are:\n\nKeyboard shortcuts\nTab completion\nUsing the bash history\n\n\n\n\nKeyboard shortcuts\nBy default, bash allows you to use many keyboard shortcuts from the GNU Emacs editor to work with your commands. These include:\n\nCtrl+a to move to the beginning of the line\nCtrl+e to move to the end of the line\nCtrl+k to cut the text from the cursor to the end of the line\nCtrl+y to paste text at the cursor\nCtrl+l to clear the terminal\n\nYou can also use Ctrl+c to terminate the command that is currently running and take you back to the command prompt.\n\n\nTab completion\nTab completion allows you to type part of a command name followed by the Tab key, which prompts bash to guess the rest of the command name based on the commands that are available on the system.\nFor instance, if you want to run the bedtools command on the server, you can type bedtoo followed by Tab, and the rest of the command name will be filled in for you.\nTyping Tab once to complete a command name only works if there is only one possible way to complete the name. If there are multiple options and you would like to see them, you can type Tab twice, and the possible completions will be displayed.\n\n\nUsing the bash history\nA particularly useful feature of command line shells is the ability to look back at previous commands that you have run. In bash you can access these commands using the history command. Typing history will show you a numbered list of commands you’ve run recently.\nThere are a few useful tricks that make it easy to find and run commands that you have run before:\n\nScrolling through the history with the up and down arrows\nSearching the history with Ctrl+r (reverse-i-search)\n\nEnter text and use Ctrl+r again to cycle through results\n\nRe-running a previously run command with ! followed by the number of the command\n\n\n\n Challenge:\n\nWhat are the possible completions of ‘bed’ on the server?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nAt the command prompt, type bed then press Tab twice. You should see many possible options.\n\n\n\n\n\n\n Challenge:\n\nUse the history to re-run the date -d '25 Dec' +%j command without re-typing it\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nThere are three ways to do this: One way would be to press Ctrl+r, then type date, then press Ctrl+r again repeatedly until you see the command, then press Enter to run it.\nAlternatively, you could run the history command and look back through the history list to find the command. Then you could type !at the command prompt followed by the number of the command in the history.\nA third way would be to either hit the up arrow to scroll back through the history until you see the command, then press Enter to run it.\n\n\n\n\n\n\n Resources\n\nAlthough Unix has thousands of commands, there are a few that are used most of the time. Even so, it can be daunting to try to memorise new unix commands. The cheat sheet below is a rely handy resource to get you started on the command line.\n\nUnix command cheat sheet"
  },
  {
    "objectID": "02-filesystem.html",
    "href": "02-filesystem.html",
    "title": "Filesystem",
    "section": "",
    "text": "In this section you will learn how to explore the Linux file system, and how to create, move, delete and edit files and directories.\n\nIntroducing the Linux file system\n\n\n\n Key Points\n\n\nIn Linux, like other operating systems, the file system is structured as a tree\n\nThe top-level directory is known as the root directory, and is referred to on the command line as /\n\nThe file system contains regular files, directories, and symbolic links to other files\n\nEach file has a unique path in the file system, along with other attributes such as its size, when it was last modified, and the permissions associated with it\n\nEach user’s files are generally stored in a directory called the user’s home directory, also referred to as ~\n\nHome directories are normally found in /home\n\nbash keeps track of the current working directory that the shell is in\n\nWhen a user logs in to a Linux system, it starts in the user’s own home directory by default\n\n\n\n\nThe Linux file system\n\n\nThe Linux file system, where all files in a Linux system are stored, is structured as a tree with a single root directory, known as /, as shown in the above image. The root directory has a number of sub-directories. The most important for us is /home as this is where users’ home directories are stored*. Each user’s files are generally stored in their home directory, and by default users are not permitted to create files outside their own home directory. You can find out the path to your home directory by running the command echo $HOME.\n\n*On the bifx servers, home directories are located at /datastore/home as home accounts are securely stored on the university DataStore cloud storage. However, /home will still work as it is set up as a shortcut.\n\n\nFile paths in Linux\n\nFile paths in Linux can be either absolute paths, or relative paths.\n\nAbsolute paths\nEach file in the Linux file system tree is uniquely identified by its absolute path. The absolute path comprises a list of the parent directories of the file, starting from the root directory, separated by the / character, followed by the name of the file. The name of a file, and the path to its parent directory, can be extracted from its path using the basename and dirname commands:\n\n[USERNAME]@bifx-core1:~$ basename /library/training/Intro_to_Linux\nIntro_to_Linux\n[USERNAME]@bifx-core1:~$ dirname /library/training/Intro_to_Linux\n/library/training\n[USERNAME]@bifx-core1:~$ \n\nIn Linux, file names can contain almost any character other than /. However, many characters, including spaces and special characters such as ’ and “, can make files difficult to work with, so, in general, it’s better to stick with letters, numbers, underscores, dashes, and dots when naming files. If you do have to work with a file that contains special characters, you can either put the file path in quotes or use backslashes to escape the special characters:\n\n[USERNAME]@bifx-core1:~$ basename /library/training/Intro_to_Linux/file name with spaces\nbasename: extra operand ‘with’\nTry 'basename --help' for more information.\n[USERNAME]@bifx-core1:~$ basename '/library/training/Intro_to_Linux/file name with spaces'\nfile name with spaces\n[USERNAME]@bifx-core1:~$ basename /library/training/Intro_to_Linux/file\\ name\\ with\\ spaces\nfile name with spaces\n[USERNAME]@bifx-core1:~$\n\nNote: Tab completion works with file and directory names as well as command names. Try to use the command above by typing basename /libr and using tab completion to find the file file name with spaces.\nThe pwd command shows the absolute path of the current working directory:\n\n[USERNAME]@bifx-core1:~$ pwd\n/datastore/home/[USERNAME]\n[USERNAME]@bifx-core1:~$\n\n\n\nRelative paths\nWhile absolute paths provide an unambiguous way of referring to files, they can be cumbersome. For this reason, Linux makes it possible to define paths relative to the current working directory or the user’s home directory:\n\n~ refers to the user’s home directory\n. refers to the current working directory\n.. refers to the parent directory of the current working directory\n\n../.. refers to the parent directory of the parent directory of the current working directory, ../../.. refers to the parent directory of that directory, and so on\n\n\nIf you just use the name of a file, Linux assumes that you are referring to a file in the current working directory.\nThe realpath command can be used to show the absolute path corresponding to a relative path:\n\n[USERNAME]@bifx-core1:~$ realpath ~\n/datastore/home/[USERNAME]\n[USERNAME]@bifx-core1:~$ realpath .\n/datastore/home/[USERNAME]\n[USERNAME]@bifx-core1:~$ realpath ..\n/datastore/home\n[USERNAME]@bifx-core1:~$ \n\n\n\nGlob patterns\nLinux also makes it possible to include wildcards in file paths, making it possible to refer to a group of file paths at once. Paths that include wildcards are called glob patterns. Useful wildcards include:\n\n* which matches any sequence of characters\n? which matches any single character\n[] which matches a single character within the square brackets\n\nfor example, [aA] would match ‘a’ or ‘A’\nranges of numbers are allowed, so [1-5] matches 1, 2, 3, 4, or 5\n\n\nWhen bash sees a glob pattern, it expands it into a list of file paths that match the pattern (separated by spaces).\nFirst, let’s look at the structure of files and folders in the directory /library/training/Intro_to_Linux/genomes by using the tree command:\n\n[USERNAME]@bifx-core1:~$ tree /library/training/Intro_to_Linux/genomes\n/library/training/Intro_to_Linux/genomes\n├── human\n│   └── hg38\n│       └── genome.fa\n└── mouse\n    ├── GRCm38\n    ├── mm10\n    │   ├── cpg_islands.bed\n    │   ├── genes.bed\n    │   ├── genome.fa\n    │   └── repeats.bed\n    ├── mm9\n    │   └── genome.fa\n    └── UCSC\n[USERNAME]@bifx-core1:~$\n\nThis contains two sub-folders, human and mouse. Within each, there are further sub-folders for different genome versions. Some of these contain files with genome sequence and annotation data.\nA convenient way to experiment with glob patterns (and to make sure they match the files you want them to) is to use the echo command, which prints its arguments to the command line. Try the following commands:\n\necho /library/training/Intro_to_Linux/genomes/*\n\necho /library/training/Intro_to_Linux/genomes/mouse/mm?\n\necho /library/training/Intro_to_Linux/genomes/mouse/mm*\n\n\n\n Challenge:\n\nCreate a glob pattern that matches /library/training/Intro_to_Linux/genomes/mouse/GRCm38 and /library/training/Intro_to_Linux/genomes/mouse/UCSC only.\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nOne option would be find folders that start with G or U:\n/library/training/Intro_to_Linux/genomes/mouse/[GU]*\nAnother is to find folders that start with a capital letter by specifying the range A-Z:\n/library/training/Intro_to_Linux/genomes/mouse/[A-Z]*\n\n\n\n\n\n\n Challenge:\n\nCreate a glob pattern that matches everything in /library/training/Intro_to_Linux/genomes/mouse except for /library/training/Intro_to_Linux/genomes/mouse/UCSC.\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nOne option would be to match folders which contain a number anywhere in the name:\n/library/training/Intro_to_Linux/genomes/mouse/*[0-9]*\n\n\n\n\n\n\nFile types and attributes in Linux\n\nThe Linux file system contains three main types of file:\n\nRegular files, which contain data\nDirectories, which contain other files or directories\nSymbolic links, which are aliases (or pointers) to files and folders\n\nAs well as its name and path, each file has a number of attributes associated with it, such as its size, when it was last modified, and the permissions associated with it. You can check the attributes associated with a file using the stat command:\n\n[USERNAME]@bifx-core1:~$ stat /library/training/Intro_to_Linux/sequences.fa\n  File: ‘sequences.fa’\n  Size: 135         Blocks: 1          IO Block: 8192   regular file\nDevice: 27h/39d Inode: 1003792241  Links: 1\nAccess: (0644/-rw-r--r--)  Uid: ( 500/user1)   Gid: (  50/group1)\nAccess: 2024-09-25 13:09:35.836470000 +0100\nModify: 2024-09-25 13:09:35.837141876 +0100\nChange: 2024-09-25 13:10:28.019841000 +0100\n Birth: -\n[USERNAME]@bifx-core1:~$\n\nThe output of the stat command shows us:\n\nWhat type of file this is (a regular file)\nThe size of the file (135 bytes)\nThe identity and group of the owner of the file (user1 and group1)\nWhen the file was last accessed, modified, and changed\nThe permissions on the file: Access: (0644/-rw-r–r–)\n\nThe permission string is -rw-r–r–\nThe first character of the permission string tells us whether it is a file or directory\nThe rest of the string can be divided into three groups (rw-, r–, and r–), representing the permissions granted to the user that owns the file, the group associated with the file, and all users\nGroups are sets of users. Permissions can be set by group to give multiple users access to the same files.\nThere are three types of permission. These are permission to read the file (r), permission to write to the file (w), and permission to execute the file (x)\n\n\nFile permissions can be altered with the chmod command. You can read more about setting permissions here.\n\n\n Challenge:\n\nWho has permission to read the file ‘/library/training/Intro_to_Linux/sequences.fa’? Who is permitted to write to it? Is anyone permitted to execute it?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nEveryone on the server can read the file. The user that owns the file can read and write to it. Nobody is permitted to execute this file.\n\n\n\n\n\n\nExploring the file system\n\n\n\n Key Points\n\n\ncd changes the current working directory\n\nUse cd path to move to a specified directory e.g. cd /library/training\nUse cd .. to move up a directory\nUse cd without an argument to return to your home directory\n\ndirs lists a history of directories you have moved between\nThe ls command lists the files in the current working directory\nThe tree command provides a readable summary of the files in the current directory and its subdirectories\nThe find command recursively searches for files in the current file system\n\n\n\nTry using the following commands to navigate within the file system, and view and find files:\n\n## Move to the Intro_to_Linux directory\ncd /library/training/Intro_to_Linux/ \n\n## List the contents of the directory\nls\n\n## List the contents of the directory including hidden files and using human readable file sizes\nls -lah\n\n## List the all files ending .gz\nls -lah *.gz\n\n## Find all files ending .bed in the mm10 genomess directory\nfind genomes/mouse/mm10/ -type f -name '*.bed'\n\n## List the contents of the directory in a tree structure\ntree\n\n## Return to the home directory\ncd\n\nNote: In this example we have used the command ls -lah. This is an example of a shorthand that you can use in the bash shell when specifying multiple flags. ls -lah is equivalent to ls -l -a -h.\n\n-l list files and show attributes\n-a show all files (including hidden files)\n-h show file size in human readable format\n\nPlay around with the ls command to see how it behaves differently without these options.\n\n\n Challenge:\n\nList all of the paths to files named ‘genome.fa’ in the directory ‘/library/training/Intro_to_Linux/genomes’\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nRun find /library/training/Intro_to_Linux/genomes -type f -name 'genome.fa'\nOr, because we know where they are in the tree:\nls /library/training/Intro_to_Linux/genomes/*/*/genome.fa\n\n\n\n\n\n\n Challenge:\n\nUsing the commands you’ve learned in this section, explore the /library/training/Intro_to_Linux/genomes/ directory on the server. Which organisms do we have genomes for? Which genome releases do we have for each of these organisms?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nRun ls /library/training/Intro_to_Linux/genomes/* to list the contents of each directory in the genomes folder.\nOR\nRun find /library/training/Intro_to_Linux/genomes/ -maxdepth 2 -type d to list the sub-directories of the directories representing the organisms, which represent genome releases.\nThere is always more than one way to do things in Linux!\n\n\n\n\n\nCreating and deleting files\n\n\n\n Key Points\n\n\nFiles can be created using:\n\ntouch, a command to create an empty file\nA Unix text editor, like emacs or vim\nBy redirecting the output of a program into a new file (we will do this later)\n\nSymbolic links can be created using ln -s\n\nA symbolic link is a shortcut to a file or directory that exists elsewhere\nOn the bifx servers /home is a symbolic link to /datastore/home\n\nCheck this by running ls -lh /home\n\n\nDirectories can be created using mkdir, and empty directories can be removed using rmdir\nThe rm command can be used to delete files, links, and directories along with their contents (using the -r flag to recursively delete the contents of a directory)\n\nThere is no recycle bin in Linux, so rm should be used with care!\nThe -i flag can be used to prompt for confirmation before deleting files\n\n\n\n\nThe following example demonstrates how we can create and remove files, directories and links:\n\n[USERNAME]@bifx-core1:~$ cd\n[USERNAME]@bifx-core1:~$ mkdir course\n[USERNAME]@bifx-core1:~$ cd course\n[USERNAME]@bifx-core1:~/course$ mkdir -p dir1 dir2 dir3/dir4\n[USERNAME]@bifx-core1:~/course$ tree\n.\n├── dir1\n├── dir2\n└── dir3\n    └── dir4\n    \n4 directories, 0 files\n[USERNAME]@bifx-core1:~/course$ touch file1\n[USERNAME]@bifx-core1:~/course$ tree\n.\n├── dir1\n├── dir2\n├── dir3\n│   └── dir4\n└── file1\n\n4 directories, 1 file\n[USERNAME]@bifx-core1:~/course$ cd dir1\n[USERNAME]@bifx-core1:~/course$ ln -s ../file1\n[USERNAME]@bifx-core1:~/course$ cd ..\n[USERNAME]@bifx-core1:~/course$ tree\n.\n├── dir1\n│   └── file1 -&gt; ../file1\n├── dir2\n├── dir3\n│   └── dir4\n└── file1\n\n4 directories, 2 files\n[USERNAME]@bifx-core1:~/course$ rmdir *\nrmdir: failed to remove 'dir1': Directory not empty\nrmdir: failed to remove 'dir3': Directory not empty\nrmdir: failed to remove 'file1': Not a directory\n[USERNAME]@bifx-core1:~/course$ tree\n.\n├── dir1\n│   └── file1 -&gt; ../file1\n├── dir3\n│   └── dir4\n└── file1\n\n3 directories, 2 files\n[USERNAME]@bifx-core1:~/course$ rm -ri *\nrm: descend into directory 'dir1'? y\nrm: remove symbolic link 'dir1/file1'? y\nrm: remove directory 'dir1'? n\nrm: descend into directory 'dir3'? y\nrm: remove directory 'dir3/dir4'? y\nrm: remove directory 'dir3'? y\nrm: remove regular empty file 'file1'? n\n[USERNAME]@bifx-core1:~/course$ tree\n.\n├── dir1\n└── file1\n\n1 directory, 1 file\n[USERNAME]@bifx-core1:~/course$ rmdir dir1\n[USERNAME]@bifx-core1:~/course$ rm -i file1\nrm: remove regular empty file 'file1'? y\n[USERNAME]@bifx-core1:~/course$ tree\n.\n\n0 directories, 0 files\n[USERNAME]@bifx-core1:~/course$\n\nThe example demonstrates a number of commands:\n\ntouch to create an empty file\n\nThis can also be used to update the timestamp on an existing file\n\nmkdir to create empty directories\n\nAdd -p when create nested directories that don’t exist yet e.g. dir3/dir4\n\nln -s to create a symbolic link to a file or directory\nrmdir to delete empty directories, without deleting files or non-empty directories\nrm command\n\nAdd -r to remove directories (and their contents)\nAdd -i to ask for confirmation before deleting\n\n\n\n\nCopying and moving files and directories\n\n\n\n Key Points\n\n\nFiles and directories can be copied using cp\n\nTo copy a directory along with its contents, use the -r flag\n\nArchive files in tar format can be extracted using the tar command\nDirectories can be synchronised using rsync, which only copies updated files\nFiles and directories can be moved or renamed using mv\nAttributes of files and directories can be changed using chmod\n\nchmod changes the permissions on a file\n\n\n\n\nNext, we will copy files and directories to our own workspace, extract archived files and update file permissions. Along the way we will see how various filesystem commands behave.\n\n## Move into your course directory\ncd ~/course\n\n## Copy the files from the training directory to your course directory\ncp /library/training/Intro_to_Linux/bioinformatics_on_the_command_line_files.tar.gz .\n\n## Extract the files from the compressed tarball archive. A tarball is a single file which contains multiple folders and files. \ntar -xzvf bioinformatics_on_the_command_line_files.tar.gz\n\n## Take a look at the output\ntree\n\n## Make a copy of the bioinformatics_on_the_command_line_files directory\ncp -r bioinformatics_on_the_command_line_files bioinformatics_on_the_command_line_files-copy\n\n## Check the contents of both directories. They should be identical.\ntree\n\n## Move the README file in the copied directory and change its name\nmv bioinformatics_on_the_command_line_files-copy/README README.txt\n\n## Inspect the result\ntree\n\n## Remove the copied directory and its contents as well as the README.txt file. Because you use -i, you will be prompted before each file is removed.\nrm -r -i bioinformatics_on_the_command_line_files-copy README.txt\n## Check the contents of the directory again\ntree\n\nThis example has demonstrated a number of commands:\n\ncp to copy files and directories (with the -r flag set)\n\nThe -a flag will preserve file attributes when they are copied\n\nmv to move files or directories\n\nThe second argument to mv is the destination. This can be:\n\nA file path to rename the file e.g. mv file1.txt ../dir1/file2.txt\nA directory path to move the file and keep the original name e.g. mv file1.txt ../dir1/\n\n\n\nFile permissions can be altered using the chmod command.\n\nIt’s always a good idea to make raw data files read only as it makes it more difficult to remove or overwrite them accidentally\nthe a-w argument removes write access for all users\nCheck the chmod man page for more details on how to change file permissions. For now it is fine to just know this is possible.\n\n\n## Inspect the file permissions of the raw data file\nls -lah bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq\n\n## Change the permissions on the file to make it read only\nchmod a-w bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq\n\n## Check the file permissions again\nls -lah bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq\n\n## If you attempt to remove the file you will get a warning that the data is read only. Hit 'n' to cancel the command.\nrm bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq"
  },
  {
    "objectID": "05-analysis.html",
    "href": "05-analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "In this section you will learn how to work with common Next Generation Sequencing (NGS) data formats on the command line.\n\nBioinformatics data formats and tools\n\n\n\n Key Points\n\n\nMany standard formats for storing high throughput sequencing data take the form of structured text files, which are easy to manipulate using standard GNU utilities (built in Unix commands)\nMany powerful specialist tools for bioinformatics analysis have been developed to use these formats\n\n\n\nMany of the most common types of file that you will have to work with as a bioinformatician take the form of structured text files, examples include:\n\nStandard formats for representing raw sequences\n\nFASTA\nFASTQ\n\nTabular formats for representing aligned reads or genome features\n\nBED\nSAM\nbedGraph\n\n\nSome other compressed formats, such as BAM, which is a compressed version of the SAM format, can easily be converted to human readable text.\nFurthermore, numerous specialist bioinformatics tools have been specifically developed for working with these file formats. For example:\n\nbedtools and bedops, which work with BED files\nVarious aligners, such as STAR, hisat2, and others, which take raw sequences in FASTQ or FASTA format and align them to the genome\nsamtools, which works with SAM files and BAM files\n\nBAM files can be viewed in SAM format using the samtools view command\n\n\nNote: These tools are not included in most Linux distributions as standard and typically have to be installed separately.\n\n\nWorking with NGS data using GNU tools\n\n\n\n Key Points\n\n\nIt is possible to perform a wide range of complex analysis tasks on NGS data files using standard GNU utilities (built in Unix commands)\nIn this section we illustrate the application of these tools to NGS data\n\n\n\nIn previous sections, we extracted an archive named bioinformatics_on_the_command_line_files.tar.gz into the ~/course directory, creating a directory called ~/course/bioinformatics_on_the_command_line_files.\nThis directory contains a file called yeast_genes.bed, which lists the genomic co-ordinates of 7,126 yeast genes in BED format, and another file called yeast_genome.fasta, which contains the yeast EF4 genome sequence in FASTA format:\n\n[USERNAME]@bifx-core1:~/course$ head -5 bioinformatics_on_the_command_line_files/yeast_genes.bed\nchrI    334 649 YAL069W .   +\nchrI    537 792 YAL068W-A   .   +\nchrI    1806    2169    YAL068C .   -\nchrI    2479    2707    YAL067W-A   .   +\nchrI    7234    9016    YAL067C .   -\n[USERNAME]@bifx-core1:~/course$ head -5 bioinformatics_on_the_command_line_files/yeast_genome.fasta\n&gt;I\nCCACACCACACCCACACACCCACACACCACACCACACACCACACCACACCCACACACACA\nCATCCTAACACTACCCTAACACAGCCCTAATCTAACCCTGGCCAACCTGTCTCTCAACTT\nACCCTCCATTACCCTGCCTCCACTCGTTACCCTGTCCCATTCAACCATACCACTCCGAAC\nCACCATCCATCCCTCTACTTACTACCACTCACCCACCGTTACCCTCCAATTACCCATATC\n[USERNAME]@bifx-core1:~/course$ \n\n\nChecking chromosome names in BED and FASTA files\nLooking at the above outputs, we can see that the naming convention for the chromosomes in the BED file (shown in the first column) appears to be different to the naming convention for the chromosomes in the FASTA file (shown in the first line after the &gt; character). In order to confirm this we would like to produce a sorted list of chromosome names in each file and compare them.\nThe example below demonstrates the following new commands:\n\ncut, which we use to select specified fields (or columns) with the -f option, and specified characters with the -c option\nsort, which sorts the lines it receives from STDIN. The -u flag tells it to remove duplicate lines from the output\ndiff, which compares two text files, and outputs the differences between them\n\nWe can do this as follows:\n\n## Move to the relevant directory\ncd ~/course/bioinformatics_on_the_command_line_files\n\n## Select the first column of the bed file, sort entries and remove duplicates. Redirect output to a file.\ncut -f1 yeast_genes.bed | sort -u &gt; yeast_genes_bed_chromosomes.list \n\n## Select the sequence ID lines from the fasta file. Sort and remove duplicates and select everything from the second character. Redirect to a file.\ngrep '^&gt;' yeast_genome.fasta | sort -u | cut -c2- &gt; yeast_genome_fasta_chromosomes.list\n\n## Check if both files have the same number of chromosomes\nwc -l *_chromosomes.list\n\n## Check for differences in the files\ndiff yeast_genes_bed_chromosomes.list yeast_genome_fasta_chromosomes.list\n\nWe can see that there is a mismatch between the chromosome names in the BED and FASTA files. Each chromosome name in the BED file is equivalent to the corresponding name in the FASTA file with ‘chr’ added to the start.\nBefore using these files in a bioinformatics analysis, we need to update one of them so that the names match. In the following example we fix the BED file by removing ‘chr’ from the start of each line.\nThe example below uses the sed command, which is used to perform a search and replace style substitution on each line in the BED file using a regular expression. Here, as in the previous examples using grep, the ‘^’ character is a regular expression character representing the start of the line. We then confirm that the chromosomes are now the same using the diff command. This produces no output, which means that there are no differences between the chromosome lists.\n\n## Remove \"chr\" from the start of every line in the bed file\nsed 's/^chr//' yeast_genes.bed &gt; yeast_genes.fixed.bed\n\n## Extract the chromsome names\ncut -f1 yeast_genes.fixed.bed | sort -u &gt; yeast_genes_fixed_bed_chromosomes.list\n\n## Check for differences\ndiff yeast_genes_fixed_bed_chromosomes.list yeast_genome_fasta_chromosomes.list\n\n## Remove the temporary list files\nrm -i *.list\n\n\n\nManipulating BED files with awk and sort\nMany standard formats for representing NGS data take the form of tabular files, in which each line contains a number of fields, separated by a particular character (generally a tab character). The yeast_genes.fixed.bed file generated in the previous example fits this pattern.\nThis example demonstrates how to use awk and sort to find the name and length of the longest gene on chromosome ‘I’ in the yeast_genes.fixed.bed file:\n\n## Use the unix programming language awk for text manipulation\nawk -F'\\t' -v OFS='\\t' '$1==\"I\" {print $4,$3-$2}' yeast_genes.fixed.bed | sort -k2,2nr | head -1\n\nThe output should be YAR050W 4614.\nThe above example showcases the power of awk in dealing with tabular data. The command awk -F'\\t' -v OFS='\\t' '$1==\"I\" {print $4,$3-$2}' can be deconstructed as follows:\n\n-F'\\t' tells awk that the field separator in the input lines is the tab character (\\t)\n-v OFS='\\t' tells awk that the tab character should also be used to separate the fields in the output file\n'$1==\"I\" {print $4,$3-$2}' is an awk program consisting of a single line. Each line of an awk program is a pair with the structure condition {action}, and the program is run on each line of the input. In this example:\n\nThe condition is $1==\"I\", which tells awk that the action should be performed if the first field is equal to “I”\nThe action is print $4,$3-$2, which tells awk to output a line to STDOUT in which the first field is the 4th field of the input line, and the second field is the result of subtracting the 2nd field from the 3rd. In this case the result is the gene length\n\n\nNote: A good resource to learn more about awk is Effective AWK Programming.\nThe sort command in the above example includes the option -k2,2nr. -k tells sort to sort by a key, which comprises a start and stop column number, followed by two options, n, which tells sort that the column contains numbers, and r, which tells sort that the lines should be sorted in reverse (i.e. descending) order.\n\n\n Challenge:\n\nYou could also remove ‘chr’ from the start of each line of yeast_genes.bed by removing the first three characters of every line. See if you can achieve this with the cut command.\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nSince we know that ‘chr’ is at the start of every line, cut -c 4- yeast_genes.bed would also work.\n\n\n\n\n\n\n Challenge:\n\nHow would you find the shortest gene on the plus strand of chromosome ‘II’ in yeast_genes.fixed.bed using awk?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nYou could run awk -F'\\t' -v OFS='\\t' '$1==\"II\"&&$6==\"+\" {print $4,$3-$2}' yeast_genes.fixed.bed | sort -k2,2n | head -1\n\n\n\n\n\n\nCase study: a simple RNA-Seq analysis workflow\n\n\n\n Key Points\n\n\nIt is possible to perform a simple bioinformatics analysis from end to end using only the bash command line\nThis section presents a simple case study using the STAR aligner and bedtools\n\n\n\nIn this section, we will work through a simple pipeline for analysing RNA-Seq data, which involves the following steps:\n\nStart with unaligned reads in FASTQ format\nAlign the reads against an index generated from a target genome, whose sequence is stored in FASTA format, obtaining an output file in BAM format\n\nHere we use the yeast EF4 genome, and align the reads using STAR\n\nCompute the genome coverage of the aligned reads, obtaining an output file in bedGraph format, which can then be viewed in a genome browser\n\nHere we generate the bedGraph file directly from the BED file using bedtools\n\nCount the overlaps between the aligned reads and genomic features, stored in BED format, obtaining an output file in BED format, and find the genes with the largest number of overlapping reads\n\nHere we use bedtools to compute the intersection between the genes and the reference, and use standard GNU utilities to find the genes with the most hits\n\n\nThis analysis uses the files in the ‘bioinformatics_on_the_command_line_files’ directory that we have already been working with, and can be performed as follows:\n\n\n## Move to the course directory\ncd ~/course\n\n## Make a new folder called analysis and move into it\nmkdir analysis\ncd analysis\n\n## Make a new folder 00_source_data and move into it\nmkdir 00_source_data\ncd 00_source_data\n\n## Link the yeast fastq file to this folder\nln -s ../../bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq \n\n## Move back to the analysis folder and inspect the directory\ncd ..\ntree\n\n## Make a directory for the STAR aligner index files and link the yeast fasta file\nmkdir 01_star_index\ncd 01_star_index\nln -s ../../bioinformatics_on_the_command_line_files/yeast_genome.fasta\n\n## Run STAR genomeGenerate to create a mapping index\nSTAR --runThreadN 5 --runMode genomeGenerate --genomeDir . --genomeFastaFiles ./yeast_genome.fasta --genomeSAindexNbases 10\ncd ..\ntree\n\n## Make a directory for the alignment outputs\nmkdir 02_aligned_reads\ncd 02_aligned_reads\n\n## Run STAR to map the reads to the yeast index\nSTAR --genomeDir ../01_star_index/ --readFilesIn ../00_source_data/raw_yeast_rnaseq_data.fastq --runThreadN 2 --outFileNamePrefix raw_yeast_rnaseq_data. --outSAMtype BAM SortedByCoordinate\ncd ..\ntree\n\n## Make a directory for coverage files\nmkdir 03_coverage\ncd 03_coverage\n\n## Use bedtools genomecov to convert bam files to bedgraph\nbedtools genomecov -ibam ../02_aligned_reads/raw_yeast_rnaseq_data.Aligned.sortedByCoord.out.bam -bg &gt; raw_yeast_rnaseq_data.genomecov.bg\ncd ..\ntree\n\n## Make a directory for counting reads over genes\nmkdir 04_gene_overlap_counts\ncd 04_gene_overlap_counts\n\n## Link the yeast gene bed file\nln -s ../../bioinformatics_on_the_command_line_files/yeast_genes.fixed.bed\n\n## Use bedtools to count intersection of genes and sequencing reads\nbedtools intersect -c -a yeast_genes.fixed.bed -b ../02_aligned_reads/raw_yeast_rnaseq_data.Aligned.sortedByCoord.out.bam | awk -F'\\t' '$7&gt;0' | sort -k7,7nr &gt; raw_yeast_overlap_data.gene_overlap_counts.bed\ncd ..\ntree\n\n## Output the 10 genes with the highest read counts\nhead -10 04_gene_overlap_counts/raw_yeast_overlap_data.gene_overlap_counts.bed\ncd ..\n\nThe output should look like this:\n\nXII 460922  466869  RDN37-2 .   -   3730\nXII 451785  457732  RDN37-1 .   -   3582\nXII 468812  468931  RDN5-2  .   +   3063\nXII 468826  468958  YLR154C-H   .   -   3063\nXII 472464  472583  RDN5-3  .   +   3060\nXII 472478  472610  YLR156C-A   .   -   3060\nXII 482044  482163  RDN5-4  .   +   3055\nXII 482058  482190  YLR157C-C   .   -   3055\nXII 485696  485815  RDN5-5  .   +   3052\nXII 485710  485842  YLR159C-A   .   -   3052\n\nNote: In this analysis we have done everything from scratch, including creating the genome index. For real analyses it is a good idea to use a pre-generated index, as indices for larger genomes take up a lot of space on the server.\n\n\n Challenge:\n\nWe saw earlier that the longest gene on chromosome I is YAR050W. Use bedtools getfasta to find the nucleotide sequence of this gene in FASTA format.\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nRunning bedtools getfasta --help shows us that we need to specify an input DNA FASTA file and a BED file with the feature co-ordinates.  We can extract the feature co-ordinates of YAR050W from yeast_genes.fixed.bed using grep or awk, and use the output of this command with bedtools getfasta. An example is illustrated in the following example:\n\n\ncd ~/course\n\ngrep \"YAR050W\" bioinformatics_on_the_command_line_files/yeast_genes.fixed.bed &gt; YAR050W.bed\n\nbedtools getfasta -fi bioinformatics_on_the_command_line_files/yeast_genome.fasta -bed YAR050W.bed\n\nThe output should look like this:\n\n&gt;I:203402-208016\nATGACAATGCCTCATCGCTATATGTTTTTGGCAGTCTTTACACTTCTGGCACTAACTA..."
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This introductory course will teach you the basics of performing bioinformatics data analysis on the command line using the GNU bash shell. It covers the following topics:\n\nGetting started with bash\nRunning commands in bash\nNavigating the file system\nManaging files, directories and links\nManaging processes\nBioinformatics data analysis with bash\nCreating bash scripts for reproducible data analysis\n\n\nThe GNU bash shell\n\nThe GNU bash shell is one of a range of command line shells that are available for UNIX based operating systems. Modern alternatives include ZSH and the fish shell. Each of these shells provides a high level interface to UNIX based operating systems such as GNU/Linux.\nCommand line shells such as bash provide similar functionality to graphical user interfaces (GUIs), such as those seen in Microsoft Windows and Mac OS X, allowing users to perform tasks such as navigating the file system, running programs, and managing processes and system settings.\n\n\nAdvantages of using a command line shell\n\nWhile command line shells such as bash are not as intuitive for beginners as the point and click interfaces offered by GUI shells, they offer a number of advantages which are very useful for Bioinformatics data analysis:\n\nFlexibility\n\nyou can log in and work on remote machines that may not have a graphical interface or remote desktop server installed\nyou can use bioinformatics tools that don’t have a GUI\n\nReliability\n\nCommand line interfaces take up less memory and system resources than graphical interfaces\nBecause of the complexity of GUI programming, tools with GUIs are more likely to contain bugs\n\nConvenience\n\nPrograms and documentation can be accessed easily\nbash keeps a history of the commands you run and makes it easy to repeat commands\n\nPower\n\nsimple programs can be combined to perform more complex functions\nyou can create scripts for data analysis, without the overhead of creating a GUI\n\n\n\n\nThe UNIX philosophy\n\nThe programs that you will use to interact with the computer using the GNU bash shell have been designed according to the UNIX philosophy, which has been summarised as follows:\n\nWrite programs that do one thing and do it well\nWrite programs to work together\nWrite programs to handle text streams, because that is a universal interface\n\nThis philosophy makes it possible to perform a wide range of complex data analysis tasks by combining a relatively small number of basic commands in different ways. Once you have mastered these basic commands, you’ll find that the bash shell provides an extremely powerful and useful way to manage bioinformatics analyses."
  },
  {
    "objectID": "07-manipulation.html",
    "href": "07-manipulation.html",
    "title": "Text Manipulation",
    "section": "",
    "text": "In this section we are going to use common Unix tools to reformat a file containing genomic annotations (GRCm39_ensembl_v110.txt).\n\nUseful Unix tools\n\n\n\n Key Points\n\n\ncut to cut specific columns or characters from a file\ngrep for pattern searching and filtering\nsed for text stream manipulation\nsort to sort files\nuniq to find unique entries\nawk and perl for programming one-liners\npaste and cat can be used to join files in different ways\nhead, tail and less are good for quick inspection\nwc to count words or lines in a file\n\n\n\nThe file GRCm39_ensembl_v110.txthas been downloaded from the Ensembl database and contains gene annotations for the GRCm39 mouse genome assembly.\nEnsembl files do not always play well with other genomic data as they use a different format. For instance, they do not use “chr” in the chromosome names and they use 1/-1 to denote strand instead of +/-.\nWe would like to reformat this file to make it compatible with other genomic datasets:\n\nAdd “chr” to the chromosome column\nReformat the strand column to contain +/-\nChange the mitochondrial contig name from MT to M\nSelect only protein coding genes\nOutput a UCSC style BED file, a standard genomic annotation format with six columns (chromosome, start, end, name, score, strand).\n\n\n\nInspecting a file\n\nWe can start by inspecting the file with the head command and see how many lines it contains with wc:\n\n## Copy the file to your course directory (~ is a shortcut to your home directory)\ncp /library/training/test_files/GRCm39_ensembl_v110.txt ~/course\n\n## Inspect the file\nwc -l GRCm39_ensembl_v110.txt\nhead GRCm39_ensembl_v110.txt \n\nThe file contains 57187 lines and has the following columns: Chromosome/scaffold name, Gene start (bp), Gene end (bp), Strand, Gene name, Gene type.\n\n\nCut, sort and uniq\n\nLet’s see which chromosomes are listed in the file. We can use cut to cut a specified column -f from our file. We then pipe the output into uniq to get unique values only.\n\n## The cut command cuts \ncut -f 1 GRCm39_ensembl_v110.txt | uniq\n\nThat didn’t look right… uniq expects sorted input. We can use sort to sort the output before piping it into uniq or use the -u flag with sort to get unique values only.\n\ncut -f 1 GRCm39_ensembl_v110.txt | sort | uniq\ncut -f 1 GRCm39_ensembl_v110.txt | sort -u\n\n\n\nPlaying with grep\n\nWe have lots of random contigs. Let’s see how many of these there are. We’ll get rid of them later and just look at the primary assembly. We can use grep to search for lines that start with “GL” or “JH” and then invert the match with -v to exclude these lines.\n\n## Lines that start with GL\ngrep \"^GL\" GRCm39_ensembl_v110.txt | wc -l\n## Lines that start with GL or JH\ngrep \"^[GL,JH]\" GRCm39_ensembl_v110.txt | wc -l\n## Lines that do not start with GL or JH\ngrep -v \"^[GL,JH]\" GRCm39_ensembl_v110.txt | wc -l\n\nWhile we’re at it, here are some other examples of grep commands:\n\n## Find the gene ENSMUSG00000064342\ngrep ENSMUSG00000064342 GRCm39_ensembl_v110.txt\n# Grep is case sensitive by default, so the next command won't find the gene\ngrep ensmusg00000064342 GRCm39_ensembl_v110.txt\n# Use the -i flag to make the search case insensitive\ngrep -i ensmusg00000064342 GRCm39_ensembl_v110.txt\n\n## Print the lines after the match\ngrep -A 2 ENSMUSG00000064342 GRCm39_ensembl_v110.txt\n## Print the lines before the match\ngrep -B 2 ENSMUSG00000064342 GRCm39_ensembl_v110.txt\n## Print the lines before and after the match\ngrep -C 2 ENSMUSG00000064342 GRCm39_ensembl_v110.txt\n\n## Do we have RNA genes in the file?\ngrep \"RNA$\" GRCm39_ensembl_v110.txt | head\n## Okay, what types of RNA genes are there?\ngrep \"RNA$\" GRCm39_ensembl_v110.txt | cut -f 6 | sort -u\n\n## Let's find the total number of genes on chromosome 1. We need to find lines that start with \"1\" followed by the tab character. Otherwise, we will get lines starting with 10, 11, 12 etc. The tab character is represented by \\t and we need to use the -P flag to enable grep to recognise this (Perl compatible regular expressions).\ngrep -P '^1\\t' GRCm39_ensembl_v110.txt | wc -l\n\n## We can use other regular expressions to pattern match specific gene names\ngrep -P 'ENSMUSG[0]+[1-3]' GRCm39_ensembl_v110.txt | head\ngrep -P 'ENSMUSG[0]{4}[1-3]' GRCm39_ensembl_v110.txt | head \n\nNow that we’ve played with the file for a bit, let’s start manipulating it to get protein coding genes in the UCSC BED format. We can use grep to search for lines that contain “protein_coding” and then invert the match with -v to exclude these lines.\n\n## Start manipulating our file\n## Protein coding genes in ucsc bed format\n\n## grep -e '^Chr' will keep the header line, we will hang onto this for now\n## grep -e 'protein_coding' will keep lines that contain 'protein_coding'\n## grep -v '^[GH,JH]' will exclude lines that start with 'GH' or 'JH' and remove random contigs\n## We will redirect the output to a new file called GRCm39_ensembl_v110.pc.txt \ngrep -e '^Chr' -e 'protein_coding' GRCm39_ensembl_v110.txt | grep -v '^[GH,JH]' &gt; GRCm39_ensembl_v110.pc.txt \n\n\n\nSed for stream editing\n\nThe sed command is a stream editor that can perform basic text transformations on an input stream. It has many uses including removing lines and performing text substitutions.\n\n## sed 1d will delete the first line, removing our header\nsed 1d GRCm39_ensembl_v110.pc.txt | head\n## sed 1,3d can delete the first three lines\nsed 1,3d GRCm39_ensembl_v110.pc.txt | head\n\n## sed -e 's/^MT/M/g' will substitute 'MT' with 'M' when found at the start of the line. Ensembl uses 'MT' to denote the mitochondrial chromosome, but UCSC use 'chrM'.\nsed -e 's/^MT/M/g' -e '1d' GRCm39_ensembl_v110.pc.txt | head\n\n## sed has a handy -i flag that allows us to edit the file in place rather than redirecting the output to a new file. Be careful with this as it will overwrite the original file. We can perform both steps by supplying multiple -e flags.\nsed -i -e 's/^MT/M/g' -e '1d' GRCm39_ensembl_v110.pc.txt\n\nWe can rerun everything we have done so far in a single command using pipes. We are also going to drop the sixth column as we don’t need the “Gene type” once we have selected protein coding genes and it is not a required column for BED format.\n\ngrep -e '^Chr' -e 'protein_coding' GRCm39_ensembl_v110.txt | grep -v '^[GH,JH]' | cut -f 1-5 | sed -e 's/^MT/M/g' -e '1d' &gt; GRCm39_ensembl_v110.pc.txt\n\n\n\nAwk and Perl for command line programming\n\nWe now need to reformat the strand column to contain +/-. We can use awk to do this. Awk is a powerful programming language that is particularly well suited to text processing and text manipulation. Awk is convenient on the command line as it can be run in one line of code.\nWe will also add “chr” to the chromosome column and reorder the columns to match the standard BED format.\n\n## -v sets variables for the awk program like OFS\n## OFS is the output field separator, we set this to a tab character\n## We use an if statement to check if the strand column is 1 or -1 and print the appropriate strand symbol\n## We then print the letters \"chr\" followed by column 1 ($1)\n## We then print columns 2, 3, 5 and a 0 for the BED score column\n## Finally, we redirect the output to a new file called GRCm39_ensembl_v110.pc.edit.bed\n\nawk -v OFS='\\t' '{ if($4==1){ print \"chr\"$1,$2,$3,$5,0,\"+\" }else{ print \"chr\"$1,$2,$3,$5,0,\"-\" }}' GRCm39_ensembl_v110.pc.txt &gt; GRCm39_ensembl_v110.pc.edit.bed\n\n## We can get the same result using Perl, an alternative to awk\nperl -lane 'if($F[3]==1){$s=\"+\";}else{$s=\"-\";}; print \"chr$F[0]\\t$F[1]\\t$F[2]\\t$F[4]\\t0\\t$s\";' GRCm39_ensembl_v110.pc.txt &gt; GRCm39_ensembl_v110.pc.pl.bed\n\nFinally, we can sort the file.\n\n## Sort -k\nhead GRCm39_ensembl_v110.pc.edit.bed\nsort GRCm39_ensembl_v110.pc.edit.bed | head\n\n## This is not what we want, take a look at the sort manual\nman sort\n\n\n\nSort using keys\n\nTo sort the file by chromosome and then by start position we can use the -k flag to set sorting keys. The first k, uses column 1 with the -V flag. This is a “version” sort which is alphanumeric and will put chr1 first. The second key, uses column 2 with -n flag. This is a numerical sort and will place lower start positions first.\nThe -k flag takes a start and end column separated by a column. It is possible to use multiple columns as a key e.g. 1,4. If you want to use a single column then repeat the column number as below.\n\nsort -k1,1V -k2,2n GRCm39_ensembl_v110.pc.edit.bed &gt; GRCm39_ensembl_v110.pc.edit.sorted.bed\n\nhead GRCm39_ensembl_v110.pc.edit.sorted.bed\n\n## Success!!!\n\nNote: Sort also has a -r flag for reverse or decreasing order."
  },
  {
    "objectID": "06-scripts.html",
    "href": "06-scripts.html",
    "title": "Scripts",
    "section": "",
    "text": "So far in this course we have learned how to work interactively on the command line to analyse data. This section will introduce the idea of bash shell scripting. We will also introduce the concept of reproducible data analysis, and will show how using scripts to analyse data can facilitate this."
  },
  {
    "objectID": "06-scripts.html#shell-scripts-in-bash",
    "href": "06-scripts.html#shell-scripts-in-bash",
    "title": "Scripts",
    "section": "Shell scripts in bash",
    "text": "Shell scripts in bash\n\n\n Key points\n\n\nA shell script is a text file containing multiple commands, which can then be run from the command line as a single command\n\n\n\nWriting and running your own scripts\nIn its most basic form, a shell script is simply a text file containing a list of commands that is run in sequence from top to bottom. This kind of script can be run by providing it as an argument to the bash command, as in the following example:\n\n[USERNAME]@bifx-core1:~$ date\nThu 12 Nov 14:21:16 GMT 2020\n[USERNAME]@bifx-core1:~$ echo hello\nhello\n[USERNAME]@bifx-core1:~$ ls /library/training/Intro_to_Linux/genomes/mouse/\nGRCm38  mm10  mm9  UCSC\n[USERNAME]@bifx-core1:~$ echo date &gt; my_script.sh\n[USERNAME]@bifx-core1:~$ echo 'echo hello' &gt;&gt; my_script.sh\n[USERNAME]@bifx-core1:~$ echo 'ls /library/training/Intro_to_Linux/genomes/mouse/' &gt;&gt; my_script.sh\n[USERNAME]@bifx-core1:~$ cat my_script.sh\ndate\necho hello\nls /library/training/Intro_to_Linux/genomes/mouse/\n[USERNAME]@bifx-core1:~$ bash my_script.sh\nThu 12 Nov 14:26:29 GMT 2020\nhello\nGRCm38  mm10  mm9  UCSC\n[USERNAME]@bifx-core1:~$\n\nIt is also possible to create a script that can be run as a standalone program rather than as an argument to bash. This involves two steps:\n\nAdd the line #!/bin/bash (known as a shebang line) as the first line of the file using a text editor\nUse the chmod command to make the file executable\n\nchmod a+x my_script.sh will change the permissions on the script so that any user can run it\n\n\nOnce these steps have been followed, it will be possible to execute the script using its path:\n\n[USERNAME]@bifx-core1:~$ cat my_script.sh\n#!/bin/bash\ndate\necho hello\nls /library/training/Intro_to_Linux/genomes/mouse/\n[USERNAME]@bifx-core1:~$ ls -l ./my_script.sh\n-rwxrwxr-x 1 [USERNAME] [USERNAME] 0 Nov 13 21:40 my_script.sh\n[USERNAME]@bifx-core1:~$ ./my_script.sh\nThu 12 Nov 14:26:29 GMT 2020\nhello\nGRCm38  mm10  mm9  UCSC\n[USERNAME]@bifx-core1:~$ \n\nNote: If you want to be able to run your script just by typing its name, you need to move it to a directory that is included in the $PATH environment variable.\n\nTo check which folders are listed in $PATH, you can type echo $PATH\nTo add a directory to $PATH permanently, add the line export PATH=[YOUR DIRECTORY]:$PATH to the end of the ~/.bashrc file, then run the command source ~/.bashrc\nIf you run a program from a directory in $PATH, it can be useful to check the full path to that program to make sure you’re not inadvertently running another program with the same name. You can do this by using the which command.\n\n\n\nEditing scripts\nYou can edit your scripts using a text editor such as emacs or vim.\n\nemacs -nw opens emacs in the terminal\n\nyou can then take the tutorial by typing Ctrl+h t\n\nvim opens vim\n\nyou can take a tutorial by running the vimtutor command at the bash prompt"
  },
  {
    "objectID": "06-scripts.html#reproducible-data-analysis-with-scripts",
    "href": "06-scripts.html#reproducible-data-analysis-with-scripts",
    "title": "Scripts",
    "section": "Reproducible data analysis with scripts",
    "text": "Reproducible data analysis with scripts\n\n\n Key points\n\n\nData analysis should be reproducible\n\nYou should be able to recreate all of the steps in your analysis\nOther researchers should also be able to recreate your analysis to verify your results\n\nThe bash shell makes it easy to analyse data interactively. However, if data is analysed this way it can be difficult to keep track of exactly which steps were taken to produce a given result\nThis problem can be solved by creating a script that contains all of the commands needed to produce the results\n\nYou and other researchers can then recreate your analysis by running the script\n\nThis section presents a bash script that replicates the case study performed in the previous section\n\n\n\nIn the previous section we showed how to perform a simple analysis of some RNA-Seq data from scratch. To turn this into a reproducible analysis workflow, it is necessary to put all of the commands that make up the analysis into a script.\n\nCreating a simple data analysis script\nThe simplest way to create a script is to put all of the commands that made up the analysis into a file in the order in which they were run, creating a file that looks like this:\n\nmkdir analysis\ncd analysis\nmkdir 00_source_data\ncd 00_source_data\nln -s ../../bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq\ncd ..\ntree\nmkdir 01_star_index\ncd 01_star_index\nln -s ../../bioinformatics_on_the_command_line_files/yeast_genome.fasta\nnice STAR --runThreadN 5 --runMode genomeGenerate --genomeDir . --genomeFastaFiles ./yeast_genome.fasta --genomeSAindexNbases 10\ncd ..\ntree\nmkdir 02_aligned_reads\ncd 02_aligned_reads\nnice STAR --genomeDir ../01_star_index/ --readFilesIn ../00_source_data/raw_yeast_rnaseq_data.fastq --runThreadN 5 --outFileNamePrefix raw_yeast_rnaseq_data. --outSAMtype BAM SortedByCoordinate\ncd ..\ntree\nmkdir 03_coverage\ncd 03_coverage\nbedtools genomecov -ibam ../02_aligned_reads/raw_yeast_rnaseq_data.Aligned.sortedByCoord.out.bam &gt; raw_yeast_rnaseq_data.genomecov.bg\ncd ..\ntree\nmkdir 04_gene_overlap_counts\ncd 04_gene_overlap_counts\nln -s ../../bioinformatics_on_the_command_line_files/yeast_genes.fixed.bed\nbedtools intersect -c -a yeast_genes.fixed.bed -b ../02_aligned_reads/raw_yeast_rnaseq_data.Aligned.sortedByCoord.out.bam | awk -F'\\t' '$7&gt;0' | sort -k7,7nr &gt; raw_yeast_overlap_data.gene_overlap_counts.bed\ncd ..\ntree\nhead -10 04_gene_overlap_counts/raw_yeast_overlap_data.gene_overlap_counts.bed\ncd ..\n\nWe can see that this works by running the above file (which is saved on the server as ‘/library/training/Intro_to_Linux/analysis_raw.sh’):\n\n## Rename the analysis folder as interactive_analysis\nmv analysis interactive_analysis\n\n## Run the bash script\nbash /library/training/Intro_to_Linux/analysis_raw.sh\n\n## Inspect the output\ntree analysis\n\nYou should see that the script runs successfully, prints messages to the screen as it goes and creates a number of output files.\n\n\nCreating an improved data analysis script\nIn this section we improve our script by doing the following:\n\nAdding a shebang line to the script so that it can be run as a standalone program\nEnsuring that the script does not keep running if one of the commands within it fails\n\nThis is what the set -euo pipefail command at the top of the script does\n\nDeleting unnecessary commands\nAdding spacing and comments to make the script easier to read\n\nIn bash, lines starting with # are taken to be comments\n\nSpecifying inputs as variables in order to make the script more general and easier to maintain, and using paths relative to ‘~’ rather than the current working directory so that the script can be run in any directory and will still find the files\nAdding a few lines to the script to create a run report, which specifies the software versions used for the analysis\n\nThe updated script is shown here:\n\n#!/bin/bash\n# This is simple RNA-Seq analysis workflow that does the following:\n# - Creates a STAR index for the genome fasta file specified in the variable GENOME_FASTA\n# - Aligns the raw reads specified in the fastq file referenced by the variable RAW_FASTQ\n# - Computes the genome coverage of the aligned reads, producing a bedGraph file\n# - Counts the overlaps over the genes specified in the file referenced by the variable GENES_BED\n\nset -euo pipefail\n\n\n# Global variables ----\n\nRAW_FASTQ='~/course/bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq'\nGENOME_FASTA='~/course/bioinformatics_on_the_command_line_files/yeast_genome.fasta'\nGENES_BED='~/course/bioinformatics_on_the_command_line_files/yeast_genes.fixed.bed'\n\n## Check files exist - this will send an error if not found\nls \"$RAW_FASTQ\" \"$GENOME_FASTA\" \"$GENES_BED\" &gt; /dev/null\n\nRAW_FASTQ_BASENAME_PREFIX=`basename $RAW_FASTQ .fastq`\nGENOME_FASTA_BASENAME=`basename $GENOME_FASTA`\n\nTHREADS=5\n\n\n# Pipeline commands ----\n\n# Create a directory for the results of the analysis\n\nmkdir analysis\ncd analysis\n\n# Link to the fastq containing the raw sequences in '00_source_data'\n\nmkdir 00_source_data\ncd 00_source_data\nln -s \"$RAW_FASTQ\"\ncd ..\n\n# Create a STAR index for the genome fasta file, storing all of the results in '01_star_index'\n\necho 'Creating STAR index...'\n\nmkdir 01_star_index\ncd 01_star_index\nln -s \"$GENOME_FASTA\"\nnice STAR --runThreadN \"$THREADS\" --runMode genomeGenerate --genomeDir . --genomeFastaFiles \"$GENOME_FASTA_BASENAME\" --genomeSAindexNbases 10\ncd ..\n\n# Align the raw sequences using STAR, storing all of the results in '02_aligned_reads'\n\necho 'Aligning raw reads...'\n\nmkdir 02_aligned_reads\ncd 02_aligned_reads\nnice STAR --genomeDir ../01_star_index/ --readFilesIn ../00_source_data/\"$RAW_FASTQ_BASENAME_PREFIX\".fastq --runThreadN \"$THREADS\" --outFileNamePrefix \"$RAW_FASTQ_BASENAME_PREFIX\". --outSAMtype BAM SortedByCoordinate\ncd ..\n\n# Create a genome coverage file in bedGraph format, and store it in '03_coverage'\n\necho 'Creating genome coverage file...'\n\nmkdir 03_coverage\ncd 03_coverage\nbedtools genomecov -ibam ../02_aligned_reads/\"$RAW_FASTQ_BASENAME_PREFIX\".Aligned.sortedByCoord.out.bam &gt; \"$RAW_FASTQ_BASENAME_PREFIX\".genomecov.bg\ncd ..\n\n# Compute the gene overlap counts, and store them as a bed file in '04_gene_overlap_counts'\n\necho 'Computing gene overlap counts...'\n\nmkdir 04_gene_overlap_counts\ncd 04_gene_overlap_counts\nln -s ../../bioinformatics_on_the_command_line_files/yeast_genes.fixed.bed\nbedtools intersect -c -a \"$GENES_BED\" -b ../02_aligned_reads/\"$RAW_FASTQ_BASENAME_PREFIX\".Aligned.sortedByCoord.out.bam | awk -F'\\t' '$7&gt;0' | sort -k7,7nr &gt; \"$RAW_FASTQ_BASENAME_PREFIX\".gene_overlap_counts.bed\ncd ..\n\n# Create a run report\n\necho `realpath $0`\" run completed successfully.\" &gt; run_report.txt\ndate &gt;&gt; run_report.txt\necho 'Software versions:' &gt;&gt; run_report.txt\necho 'STAR' &gt;&gt; run_report.txt\nSTAR --version &gt;&gt; run_report.txt\necho 'bedtools' &gt;&gt; run_report.txt\nbedtools --version | sed 's/^bedtools //' &gt;&gt; run_report.txt\necho 'sort' &gt;&gt; run_report.txt\nsort --version | sed 's/^sort //' | head -1 &gt;&gt; run_report.txt\necho 'awk' &gt;&gt; run_report.txt\nawk --version | head -1 &gt;&gt; run_report.txt\n\n# If we've got here the pipeline has completed successfully\n\necho 'Pipeline completed successfully.'\n\nAgain, we can see that this works by running the above file (which is saved on the server as ‘/library/training/Intr_to_Linux/analysis_improved.sh’):\n\n## Rename the analysis folder as analysis_raw\nmv analysis analysis_raw\n\n## Run the improved bash script\nbash /library/training/Intro_to_Linux/analysis_improved.sh\n\n## Read the output report\ncat analysis/run_report.txt\n\n## Inspect the output\ntree analysis\n\n\n\nManaging your scripts\nOnce you start writing scripts, it is a good idea to use a version control system to keep track of the changes you make to your scripts. A good choice for this would be git. The following five commands will allow you to use git for version control:\n\ngit init to initialise a git repository in the current working directory\ngit add -A; git commit -m \"Latest updates\" to commit any changes to files in the current working directory to the repository\ngit status to see if any changes have been made since the last commit\ngit diff to see the differences between the files in the current working directory and the last commit\ngit show HEAD:[NAME OF FILE IN GIT] to view the most recent version of a file in the git repository\n\nYou can save the output of git show to a file by redirecting its output using &gt;\n\n\nTo learn more about git, you can look at this software carpentry course, which covers it in a lot more detail.\nNote: git should only be used for small text files such as scripts. It is not designed to work with large data files."
  },
  {
    "objectID": "06-scripts.html#closing-thoughts-practical-workflows-on-the-command-line",
    "href": "06-scripts.html#closing-thoughts-practical-workflows-on-the-command-line",
    "title": "Scripts",
    "section": "Closing thoughts: practical workflows on the command line",
    "text": "Closing thoughts: practical workflows on the command line\nIn this section we created a data analysis script in bash that satisfies two major requirements for reproducible data analysis:\n\nThe script provides a complete and accurate record of the steps that were taken to produce the output files, along with a record of the versions of the software tools used to manipulate the data\nThe script is also in a form that would be easy to share with other researchers, allowing them to replicate your analysis easily\n\nThe main drawback of using bash scripts to represent workflows is that they are often impractical. The example presented here uses an extremely small input FASTQ file, and an organism with a relatively small genome. As a result it can be run from scratch in under a minute. Running the same pipeline with a realistically sized input FASTQ file and a larger genome could take hours to run, so it is no longer practical to re-run the pipeline every time you make a change. A workaround for this would be to comment out parts of the script that you do not want to re-run, however this is considered bad practice as it introduces the possibility of human error in selecting the parts of the pipeline that need to be re-run when the pipeline is updated.\nIf you are writing analysis pipelines, you should look into using a modern workflow system such as Snakemake or Nextflow. Workflows written for these workflow systems can be run repeatedly, and the workflow system will automatically work out which steps of the analysis workflow need to be re-run based on the timestamps of the files. They also provide a number of other useful features:\n\nThey work out which steps of the analysis workflow can be run in parallel, and assign steps to different cores when possible, which can speed up the analysis considerably\nThey simplify the process of running workflows on a computing cluster"
  },
  {
    "objectID": "04-processes.html",
    "href": "04-processes.html",
    "title": "Processes",
    "section": "",
    "text": "In this section you will learn how to work with processes and jobs in bash."
  },
  {
    "objectID": "04-processes.html#exploring-processes",
    "href": "04-processes.html#exploring-processes",
    "title": "Processes",
    "section": "Exploring processes",
    "text": "Exploring processes\n\n\n Key points\n\n\nA running program is known as a process. On a modern computer many different processes can run at once\nProcesses are managed by the operating system (Linux in our case)\n\nThe operating system controls how the computer’s resources, such as CPU and disk access, are allocated to the different processes\n\n\n\n\nThere are a few different ways to explore the processes running on your computer:\n\nps shows the processes you are currently running\ntop shows all active processes\nhtop gives a user friendly representation of the processes currently running along with CPU and memory usage on the server\npgrep finds the process IDs of all running instances of a particular program\n\nWe can learn a lot about processes by looking at the output of these commands. For example, if we run the command ps -fly we get something like this:\n\n[USERNAME]@bifx-core1:~/course$ ps -fly\nS UID         PID   PPID   C  PRI  NI   RSS   SZ    WCHAN  STIME TTY          TIME CMD\nS [USERNAME]  35269 35244  0  80   0    39296 13648 wait   Nov10 pts/5    00:00:04 -bash\nR [USERNAME]  44935 35269  0  80   0    3328  8822  -      10:11 pts/5    00:00:00 ps -fly\n...\n\nBy looking at the output columns, we can see the following:\n\nEach process has an owner, whose username is shown in the UID column\nEach process has a unique ID, shown in the PID column\nEach process also has a single parent process, whose ID is shown in the PPID column\n\nThe parent of a process is the process that started it. In our case, the ps -fly process was started by the bash process\nYou can see this by observing that the PPID of the ps -fly process is the same as the PID of the bash process\n\n\n\n\n Challenge:\n\nUse htop to find the following:\n\n\nHow much memory does the server have?\n\n\nWhat is the process ID of the htop process? Use the -u argument plus your username to only list your own processes.\n\n\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nRun htop -u [USERNAME] to start htop and display only your own processes. At the top of the htop screen there is a bar chart style representation of average CPU and memory load. The server has 503G of memory. The process ID of htop is in the first column."
  },
  {
    "objectID": "04-processes.html#job-control",
    "href": "04-processes.html#job-control",
    "title": "Processes",
    "section": "Job control",
    "text": "Job control\n\n\n Key points\n\n\nA ‘job’ is a process that is managed by the bash shell, and is a child of the bash process\n\nIn general, whenever you run a command in bash, it starts a job\n\nThe shell keeps track of all of the jobs that it’s currently managing\nRunning a command starts it as a foreground job\n\nWhen a job is run in the foreground you are not shown another command prompt until the job completes.\n\nRunning a command followed by & starts it as a background job\n\nWhen a job is run in the background you are shown another command prompt immediately.\n\n\n\n\nControlling processes and jobs\n\nIn UNIX based systems, jobs and processes can be paused or stopped completely by sending them ‘signals’. There are many signals that you can send (type kill -l to list them all). Arguably the most important are the following:\n\nSIGINT, SIGHUP, and SIGTERM, which request that the process should terminate gracefully\n\nNot all programs respond to these signals.\n\nSIGKILL, which terminates the process immediately\n\nThis should only be used as a last resort, as processes that are sent this signal will not be able close gracefully\n\nSIGTSTP, which suspends the process, and SIGCONT, which restarts a suspended process\n\n\nSending signals to jobs or processes\nThere are many ways to send signals to jobs. Here are some particularly useful ones:\n\nThe kill command sends a specified signal to an individual process or job by ID\n\njobs shows you the names and IDs of all of the jobs that you are currently running in your shell\nYou can also use killall to send a signal to all running instances of a given program, but this is generally a bad idea. It is better to use htop or a combination of pgrep and kill to ensure you only send a signal to the process that you want to\n\nPressing f9 in htop also allows you to send the signal of your choice to the selected process\n\nThere are also some useful keyboard shortcuts to send signals to jobs\n\nCtrl+c sends the SIGINT signal to the current foreground job\nCtrl+z sends the SIGTSTP signal to the current foreground job\n\n\n\nRestarting jobs\nOnce a job has been stopped, it can be restarted in a number of ways:\n\nfg [job ID] restarts a job in the foreground\nbg [job ID] restarts a job in the background\nSend the SIGCONT signal to the job using kill -CONT [job or process ID] or htop\n\n\n\nDisowning jobs\nBy default, jobs you start in your shell are ‘owned’ by your current shell session. As a result, on some servers the job might be terminated when you exit the shell (depending on how bash is configured). You can ensure that this does not happen by ‘disowning’ the job.\n\ndisown [job ID] disowns a currently running job\n\nonce a job has been disowned by the shell, it will disappear from the shell’s job table. The process will become a child of the top level process\n\nnohup can be used to start a job that will definitely not be sent a SIGHUP signal when the shell is closed\n\nUsing nohup also diverts any output that would have gone to the shell to a file called nohup.out\n\n\nThe following example shows how to move a foreground job to the background and disown it:\n\n[USERNAME]@bifx-core1:~/course$ sleep 10000\n&lt;Ctrl+z&gt;\n^Z\n[1]+  Stopped                 sleep 10000\n[USERNAME]@bifx-core1:~/course$ bg %1\n[1]+ sleep 10000 &\n[USERNAME]@bifx-core1:~/course$ jobs\n[1]+  Running                 sleep 10000 &\n[USERNAME]@bifx-core1:~/course$ disown %1\n[USERNAME]@bifx-core1:~/course$ jobs\n[USERNAME]@bifx-core1:~/course$ ps\n  PID TTY          TIME CMD\n29621 pts/1    00:00:00 sleep\n37480 pts/1    00:00:00 bash\n40951 pts/1    00:00:00 ps\n[USERNAME]@bifx-core1:~/course$ \n\nThe example shows that when the sleep 10000 job is disowned it is removed from the jobs list, but the process keeps running.\n\n\n Challenge:\n\nHow could you terminate a foreground job without using Ctrl+c?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nOne way would be to pause the job with Ctrl+z, and then use kill -INT [job ID] to send a SIGINT signal to the job\n\n\n\n\n\n\n Challenge:\n\nHow could you restart a stopped job that has been disowned?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nYou could use htop or ps to find the process IDs of the stopped processes, and then use kill -CONT [process ID] to send a SIGCONT signal to those processes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Linux",
    "section": "",
    "text": "This is the homepage for the Introduction to Linux course run by the Bioinformatics Core at the Discovery Research Platform for Hidden Cell Biology.\n\n\n\nContact shaun.webb@ed.ac.uk for more information."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "03-files.html",
    "href": "03-files.html",
    "title": "Files",
    "section": "",
    "text": "In this section you will learn how to explore and manipulate files in bash using simple commands, and compound commands using pipes.\n\nExploring files\n\n\n\n Key Points\n\n\nRegular files in Linux can be classified as text files, which contain human readable text, and binary files, that contain data that is not human readable\nThe cat command can be used to show the contents of a file\n\nThe less command allows you to page through a large file\n\nHit q to exit the less command\n\n\nThe head and tail commands can be used to show the first or last few lines of a file\n\nThese can be useful for large text files\n\nThe wc -l command counts the number of lines in a file\nThe grep command allows you to filter a text file\nText files can be compressed using the gzip command, which converts them to a binary format that takes up less space\n\nMany of the above commands for working with text files have equivalents for gzipped files\nThese include zcat, zless, and zgrep\n\n\n\n\nThe following commands can be used to explore text files:\n\n## Ensure you are in the course directory\ncd ~/course\n\n## Inspect the contents\ntree\n\n## The cat command prints everything to screen\ncat bioinformatics_on_the_command_line_files/README\n\n## The less command prints one page at a time. Use space to tab through pages and `q` to quit.\nless bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq\n\n## Print the first 5 lines of a file\nhead -5 bioinformatics_on_the_command_line_files/yeast_genome.fasta\n\n## Print the final 5 lines of a file\ntail -5 bioinformatics_on_the_command_line_files/yeast_genome.fasta\n\n## How many lines are in the file?\nwc -l bioinformatics_on_the_command_line_files/yeast_genes.bed\n\nThe grep command can search and filter files:\n\ngrep expression filename returns all the lines with the word ‘expression’ in the file called ‘filename’.\ngrep -i is case insensitive\ngrep -c will count the number of lines matching an expression\ngrep -v returns all of the lines that do not match an expression\n\n\n## Print lines containing the word 'format'\ngrep format bioinformatics_on_the_command_line_files/README\n\n## Print lines not containing the word 'format'\ngrep -v format bioinformatics_on_the_command_line_files/README\n\n## Count the lines beginning with \"&gt;\". All sequence ID lines start with a \"&gt;\" in a fasta file. So we can use this command to count the number of sequences.\ngrep -c '^&gt;' bioinformatics_on_the_command_line_files/yeast_genome.fasta\n\nNote: grep allows you to use a regular expressions to specify a pattern that grep will look for rather than a fixed string. Conceptually, regular expressions are similar to glob patterns, although their syntax is different. Some characters have a special meaning in regular expressions. For example:\n\n^ represents the start of a string\n$ represents the end of a string\n\nThe -E and -P flags can be used to make more regular expressions available to grep.\nFiles can be zipped (compressed) and unzipped (decompressed) with the gzip command:\n\ngzip -k will compress a file and keep the original uncompressed version\ngzip -d will decompress a gzip’d file\nzless, zcat and zgrep work on compressed files\n\n\n## Compress the file while keeping the original uncompressed version\ngzip -k bioinformatics_on_the_command_line_files/yeast_genome.fasta\n\n## Inspect both files\nls -lh bioinformatics_on_the_command_line_files/yeast_genome.fasta*\n\n## Use zgrep to search a compressed file\nzgrep -c -E '^&gt;' bioinformatics_on_the_command_line_files/yeast_genome.fasta.gz\n\n\n\n Challenge:\n\nHow would you check that every line of the ‘yeast_genes.bed’ file starts with the string ‘chr’ without looking through the whole file?\n\n\nSolution\n\n\nSolution. \n\n Solution:\n\nRun grep -v -c -E '^chr' bioinformatics_on_the_command_line_files/yeast_genes.bed to count the number of lines that don’t start with ‘chr’. We can see that this is zero, so every line must start with ‘chr’.\n\n\n\n\n\nShell redirection\n\n\n\n Key Points\n\n\nThe shell can manage where programs receive inputs from and where they send outputs to\nIt provides three I/O channels for programs to use. These are:\n\nStandard input, or STDIN, which provides input to the program\nStandard output, or STDOUT, which receives output from the program\nStandard error, or STDERR, which receives error messages from the program\n\nProgram authors don’t have to use these I/O channels, but most command line tools designed for Linux, such as the GNU coreutils, do use them\nBy default, STDIN comes from the keyboard, and STDOUT and STDERR go to the terminal, but each of these channels can be redirected\n\n&gt; redirects STDOUT to an output file, overwriting its contents\n&gt;&gt; redirects STDOUT to an output file, appending to its contents\n2&gt; redirects STDERR to an output file, overwriting its contents\n2&gt;&gt; redirects STDERR to an output file, appending to its contents\n&lt; reads each line from an input file and feeds it to STDIN\n2&gt;&1 redirects STDERR to STDOUT\n\n\n\n\nThe following examples demonstrate how shell redirection works:\n\n## Direct the output of the echo command to a new file called output.txt\necho zero &gt; output.txt\n\n## Direct the file as input to the cat command. You could also just run: cat output.txt\ncat &lt; output.txt\n\n## Overwrite the file with the new output\necho one &gt; output.txt\ncat &lt; output.txt\n\n## Append output to the existing file\necho two &gt;&gt; output.txt\ncat &lt; output.txt\n\n## Redirect std output and std error to different files\ncat bioinformatics_on_the_command_line_files/README &gt; cat_readme.out 2&gt; cat_readme.err\n## Check the contents of these files. Did the command run without errors?\nhead -2 cat_readme.*\n\n## Redirect std output and std error to different files\nzcat bioinformatics_on_the_command_line_files/README &gt; zcat_readme.out 2&gt; zcat_readme.err\n## Check the contents of these files. Did the command run without errors?\nhead -2 zcat_readme.*\n\n## Redirect std output and std error to the same file\nzcat bioinformatics_on_the_command_line_files/README &gt; zcat_readme.all 2&gt;&1\ncat zcat_readme.all\n\n## Remove the temporary files you just created\nrm -i *cat_readme.* output.txt\n \n\n\n\nCreating compound commands using pipes\n\n\n\n Key Points\n\n\nBecause the shell provides standard input and output channels, it is possible to chain together simple commands to perform complex tasks\nThis can be done using a ‘pipe’, represented by the pipe character |\n\n\n\nSo far we have discussed simple commands, which consist of a single command name followed by some options and arguments. However, a lot of the flexibility of the tools accessible via bash comes from the ability to combine them to form compound commands, using pipes. This allows the user to perform complex tasks by joining together simple commands.\nMotivating example: How do you count how many of the first 40 lines in a FASTQ file contain the sequence ACTG?\nHere’s how you could do it using simple commands:\n\n## Get the first 40 lines of the file and save to a temporary file\nhead -40 bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq &gt; first_40_lines.tmp\n\n## Count the number of lines containing ACTG\ngrep -c ACTG first_40_lines.tmp\n\n## Remove the temporary file\nrm -i first_40_lines.tmp\n\nThe answer should be 5. Here’s how you can do it in a single command using a pipe:\n\nhead -40 bioinformatics_on_the_command_line_files/raw_yeast_rnaseq_data.fastq | grep -c ACTG\n\nPipes are particularly useful for working with large files, as they remove the need to create large intermediate files, which may take up space. They can also save time, as commands can sometimes start working on the data produced by commands preceding them in the pipeline before they have finished running, and in some cases preceding commands can be terminated early if further outputs are no longer needed.\n\n\n Example\n\nIn the example below we measure the time it takes to find the first line that contains the character ‘A’ in a large genome file. We first write all of the matches to a temporary file and then find the first line using head as follows:\n\n[USERNAME]@bifx-core1:~/course$ time grep A /datastore/home/genomes/mouse/UCSC/mm10/Sequence/WholeGenomeFasta/genome.fa &gt; grepA.tmp\n\nreal    0m36.182s\nuser    0m7.831s\nsys     0m6.377s\n[USERNAME]@bifx-core1:~/course$ time head -1 grepA.tmp\ngcttcagaataatcatattattctcaaattttgtatcaatataaaaaaaA\n\nreal    0m0.008s\nuser    0m0.001s\nsys     0m0.003s\n[USERNAME]@bifx-core1:~/course$ rm -ri grepA.tmp\nrm: remove regular file 'grepA.tmp'? y\n[USERNAME]@bifx-core1:~/course$ \n\nWe can see that the operation takes just over 36 seconds. In the example below, we pipe the output of the grep command into the head command.\n\n[USERNAME]@bifx-core1:~/course$ time grep A /datastore/home/genomes/mouse/UCSC/mm10/Sequence/WholeGenomeFasta/genome.fa | head -1\ngcttcagaataatcatattattctcaaattttgtatcaatataaaaaaaA\n\nreal    0m0.011s\nuser    0m0.002s\nsys 0m0.012s\n\nAs we can see the operation completes in about 0.01 seconds, so using a pipe is considerably faster. This is because the pipeline stops when it has found the first match, so the grep command doesn’t have to go through the whole file."
  }
]